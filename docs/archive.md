# Data Archiving

When the crawls are complete, the data is archived to several places:

- Internet Archive (sqldump)
- HTTP Archive (sqldump, csv)
- Google Storage (har)
- BigQuery (har)

## Archiving to Internet Archive

TODO

## Archiving to HTTP Archive

The [downloads](http://httparchive.org/downloads.php) page provides links to data dumps for all crawls in MySQL and CSV formats.

These files are generated by [bulktest/update.php](https://github.com/HTTPArchive/httparchive/blob/814502e7ac84f61c46b96275bce1b4cf092150af/bulktest/update.php#L215-L257) using the `mysqldump` tool.

## Archiving to Google Storage

TODO

Pro tip: To copy a handful of files from one GCS bucket to another:

```
gsutil ls gs://httparchive/[source bucket] | head -10 | gsutil cp -I gs://httparchive/[destination bucket]
```

where `head -10` controls how many files to copy.

## Archiving to BigQuery

See the [BigQuery](bigquery.md) docs.
